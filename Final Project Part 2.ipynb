{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e0b5870",
   "metadata": {},
   "source": [
    "## Final Project Part 2 - NLP Tasks\n",
    "#### Josh Breunig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61b6097",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "931ac262",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d7b3f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading in the review data in chunks\n",
    "reviews_df = pd.read_json('Data/yelp_academic_dataset_review.json', lines=True, chunksize=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82de14fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterating through the data and pulling the desired columns into a data frame\n",
    "reviews_output = pd.DataFrame()\n",
    "for chunk in reviews_df:\n",
    "    data = chunk[['business_id', 'stars', 'text']]\n",
    "    reviews_output = pd.concat([reviews_output, data], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5569b961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding sentiment labels to the dataframe based on user 'star' rating\n",
    "reviews_output['sentiment'] = ['positive' if stars >= 3 else 'negative' for stars in reviews_output['stars']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df37f824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cutting the size of the dataset\n",
    "reviews_output = reviews_output[:200000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5c614e",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "We will be using the normalize_corpus function from Sarkar's book to preprocess the reviews.  The following preprocessing steps will be performed:\n",
    "- Correct spelling\n",
    "- Lemmatization\n",
    "- Remove whitespace\n",
    "- Remove numbers and special characters\n",
    "- Remove stop words (We will want to edit the stop words list and remove words like 'no', 'not', etc. to ensure we capture the context of each review)\n",
    "- Expand contractions\n",
    "- Remove accented characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "901b862e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07dd031a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(doc, text_lemmatization=True, stopword_removal=True):\n",
    "   # adjusting the stop word list\n",
    "    stop_words = nltk.corpus.stopwords.words('english')\n",
    "    stop_words.remove('no')\n",
    "    stop_words.remove('but')\n",
    "    stop_words.remove('not')\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    normalized_text = []\n",
    "    for text in doc:\n",
    "        text = text.lower()\n",
    "        text = text.strip()\n",
    "        text = re.sub(r'[\\r|\\n|\\r\\n]', '', text) # removing html tags\n",
    "        text = re.sub(r'[^a-zA-Z0-9]', ' ', text) # removing special characters\n",
    "        text = re.sub(r'[0-9]', '', text) # removing numbers\n",
    "        if text_lemmatization:\n",
    "            tokens = word_tokenize(text)\n",
    "            tokens = [token.strip() for token in tokens] \n",
    "            text = ' '.join([lemmatizer.lemmatize(w) for w in tokens])\n",
    "        if stopword_removal:\n",
    "            filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "            text = ' '.join(filtered_tokens)\n",
    "        # correct word lengthening\n",
    "        pattern = re.compile(r'(.)\\1{2,}')\n",
    "        text = pattern.sub(r'\\1\\1', text)\n",
    "        normalized_text.append(text)\n",
    "    return normalized_text\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0283bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the reviews to an array\n",
    "reviews = np.array(reviews_output['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d08ec23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing the reviews\n",
    "norm_reviews = normalize_data(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27dbbdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the normalized reviews to a dataframe\n",
    "norm_reviews_df = pd.DataFrame(norm_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50e50e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading in csv with normalized reviews\n",
    "norm_reviews_df = pd.read_csv(\"preprocessed_reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f76a8459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>norm_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>delicious best sandwich shop ever way top sand...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>terrific unique little spot downtown tucson gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>company moved irvington area darling husband a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>appointment today truly nolen not show time bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>came sushi rose celebrate friend birthday free...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           norm_text\n",
       "0  delicious best sandwich shop ever way top sand...\n",
       "1  terrific unique little spot downtown tucson gr...\n",
       "2  company moved irvington area darling husband a...\n",
       "3  appointment today truly nolen not show time bu...\n",
       "4  came sushi rose celebrate friend birthday free..."
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_reviews_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b08457fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_reviews_df.rename(columns= {'0':'norm_text'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a7727fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = reviews_output.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5218b4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df['norm_text'] = norm_reviews_df['norm_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ec3d507e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>norm_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XQfwVwDr-v0ZS3_CbbE5Xw</td>\n",
       "      <td>3</td>\n",
       "      <td>If you decide to eat here, just be aware it is...</td>\n",
       "      <td>positive</td>\n",
       "      <td>delicious best sandwich shop ever way top sand...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7ATYjTIgM3jUlt4UM3IypQ</td>\n",
       "      <td>5</td>\n",
       "      <td>I've taken a lot of spin classes over the year...</td>\n",
       "      <td>positive</td>\n",
       "      <td>terrific unique little spot downtown tucson gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>YjUWPpI6HXG530lwP-fb2A</td>\n",
       "      <td>3</td>\n",
       "      <td>Family diner. Had the buffet. Eclectic assortm...</td>\n",
       "      <td>positive</td>\n",
       "      <td>company moved irvington area darling husband a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kxX2SOes4o-D3ZQBkiMRfA</td>\n",
       "      <td>5</td>\n",
       "      <td>Wow!  Yummy, different,  delicious.   Our favo...</td>\n",
       "      <td>positive</td>\n",
       "      <td>appointment today truly nolen not show time bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>e4Vwtrqf-wpJfwesgvdgxQ</td>\n",
       "      <td>4</td>\n",
       "      <td>Cute interior and owner (?) gave us tour of up...</td>\n",
       "      <td>positive</td>\n",
       "      <td>came sushi rose celebrate friend birthday free...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id  stars  \\\n",
       "0  XQfwVwDr-v0ZS3_CbbE5Xw      3   \n",
       "1  7ATYjTIgM3jUlt4UM3IypQ      5   \n",
       "2  YjUWPpI6HXG530lwP-fb2A      3   \n",
       "3  kxX2SOes4o-D3ZQBkiMRfA      5   \n",
       "4  e4Vwtrqf-wpJfwesgvdgxQ      4   \n",
       "\n",
       "                                                text sentiment  \\\n",
       "0  If you decide to eat here, just be aware it is...  positive   \n",
       "1  I've taken a lot of spin classes over the year...  positive   \n",
       "2  Family diner. Had the buffet. Eclectic assortm...  positive   \n",
       "3  Wow!  Yummy, different,  delicious.   Our favo...  positive   \n",
       "4  Cute interior and owner (?) gave us tour of up...  positive   \n",
       "\n",
       "                                           norm_text  \n",
       "0  delicious best sandwich shop ever way top sand...  \n",
       "1  terrific unique little spot downtown tucson gr...  \n",
       "2  company moved irvington area darling husband a...  \n",
       "3  appointment today truly nolen not show time bu...  \n",
       "4  came sushi rose celebrate friend birthday free...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cbe164a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving  to a csv\n",
    "full_df.to_csv(\"preprocessed_df.csv\", index=False)\n",
    "full_df.fillna(\" \", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ff8843",
   "metadata": {},
   "source": [
    "### Create Test/Train Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "28555d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_array = np.array(full_df['norm_text'])\n",
    "sentiments_array = np.array(full_df['sentiment'])\n",
    "\n",
    "# calculating the 33% mark to split the data into train and test datasets\n",
    "total_reviews = full_df.shape[0]\n",
    "cut_off = round(total_reviews*.33)\n",
    "\n",
    "# build train and test datasets\n",
    "train_reviews = reviews[cut_off:]\n",
    "train_sentiments = sentiments_array[cut_off:]\n",
    "test_reviews = reviews_array[:cut_off]\n",
    "test_sentiments = sentiments_array[:cut_off]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "67d227fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Dataset Shape:  (66000,)\n",
      "Train Dataset Shape:  (134000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Dataset Shape: \", test_reviews.shape)\n",
    "print(\"Train Dataset Shape: \", train_reviews.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c5ae7f",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f3fe0c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# build BOW features on train_reviews\n",
    "cv = CountVectorizer(binary=False, min_df=0.0, max_df=1.0, ngram_range=(1,2))\n",
    "cv_train_features = cv.fit_transform(train_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2e1e26eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform test reviews into features\n",
    "cv_test_features = cv.transform(test_reviews)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "df3d038d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving vectorizer \n",
    "pickle.dump(cv, open(\"cv.pickel\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c542097",
   "metadata": {},
   "source": [
    "### Sentiment Analysis\n",
    "#### Model Training, Prediction, and Performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f31e38",
   "metadata": {},
   "source": [
    "##### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d86acc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(penalty='l2', max_iter=700, C=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1f07d8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(cv_train_features, train_sentiments)\n",
    "lr_predictions = lr.predict(cv_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "84e49783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance metrics:\n",
      "------------------------------\n",
      "Accuracy: 0.9265\n",
      "Precision: 0.9242\n",
      "Recall: 0.9265\n",
      "F1 Score: 0.9236\n",
      "\n",
      "Model Classification report:\n",
      "------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.94      0.98      0.96     53550\n",
      "    negative       0.87      0.72      0.79     12450\n",
      "\n",
      "    accuracy                           0.93     66000\n",
      "   macro avg       0.90      0.85      0.87     66000\n",
      "weighted avg       0.92      0.93      0.92     66000\n",
      "\n",
      "\n",
      "Prediction Confusion Matrix:\n",
      "------------------------------\n",
      "                 Predicted:         \n",
      "                   positive negative\n",
      "Actual: positive      52220     1330\n",
      "        negative       3524     8926\n"
     ]
    }
   ],
   "source": [
    "import model_evaluation_utils as meu\n",
    "\n",
    "meu.display_model_performance_metrics(true_labels=test_sentiments, \n",
    "                                      predicted_labels=lr_predictions,\n",
    "                                     classes=['positive', 'negative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8fd82970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model traning and saving\n",
    "import pickle\n",
    "pickle.dump(lr, open('saved_lr_model.sav', 'wb')) # saving the LR model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef08f9a",
   "metadata": {},
   "source": [
    "### Contribution Statement\n",
    "\n",
    "I managed all contributions for this sprint, as I am working on this project solo.\n",
    "\n",
    "One of the biggest challenges I faced during this sprint was the size of the dataset.  With over 6M data points and a file size of ~5GB, it was very difficult to work with the data on my local machine.  It was a challenge to load the data into the program, but I was able to find a solution by chunking the file.  I was very limited by the preprocessing steps I was able to take… the more steps included in the function, the longer it would take to run.  I originally tried to use the text_normalizer.py file from Sarkar’s book and that took over nine hours to run and eventually failed.  In the end, I decided to cut down the size of the dataset to make it more manageable. \n",
    "\n",
    "Another issue I faced with the preprocessing step was expanding the contractions.  Breaking down the contractions could help the sentiment analysis results, but I wasn’t able to successfully implement that step into my preprocessing code.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
